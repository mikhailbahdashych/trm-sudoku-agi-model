# TRM for Sudoku-Extreme Configuration
# Optimized for L4 GPU (24GB VRAM)
# Based on "Less is More: Recursive Reasoning with Tiny Networks"

# Model Architecture
model:
  hidden_dim: 512          # Hidden dimension (paper: 512)
  n_latent: 6              # Latent recursions per deep step (paper: 6)
  T_deep: 3                # Deep recursion loops (paper: 3)
  use_act: true            # Adaptive Computation Time
  mlp_ratio: 2.666667      # 8/3 for SwiGLU

# Training Configuration
training:
  # Epochs and samples
  epochs: 400              # Reduced (99% accuracy reached at ~350 with these settings)
  train_samples: 1000      # Same as paper (small-sample learning)
  augmentations_per_sample: 200  # Paper: 1000, gives 200K samples/epoch

  # Optimization (tuned for batch_size=8192)
  # With 200K samples / 8192 batch = ~24 steps per epoch
  batch_size: 8192         # Optimized for L4 GPU (~10GB VRAM usage)
  learning_rate: 0.002     # Scaled up: larger batch needs higher LR
  weight_decay: 0.1        # Reduced from 1.0 (paper was for 60K epochs)
  warmup_steps: 200        # ~8 epochs warmup (200 steps / 24 steps per epoch)
  max_grad_norm: 1.0       # Gradient clipping

  # EMA
  ema_decay: 0.999         # Same as paper
  ema_warmup_steps: 50     # Reduced proportionally

  # Mixed precision
  use_amp: true
  amp_dtype: float16       # L4 works better with FP16 than BF16

# Data
data:
  dataset_name: "Ritvik19/Sudoku-Dataset"
  num_workers: 4
  seed: 42
  test_samples: 1000       # Limit test samples to avoid memory issues

# Logging and Checkpointing
# Adjusted for ~24 steps per epoch
logging:
  log_interval: 24         # Log once per epoch
  eval_interval: 120       # Evaluate every ~5 epochs
  save_interval: 240       # Save checkpoint every ~10 epochs
  output_dir: outputs

# Evaluation
evaluation:
  test_samples: null       # null = all test samples
  llm_comparison_samples: 5

# LLM Comparison (Ollama)
llm:
  model: "llama3.2"        # or "mistral", "qwen2.5"
  ollama_url: "http://localhost:11434"
  strategies:
    - direct
    - cot
    - fewshot

# Device
device: cuda               # cuda or cpu
