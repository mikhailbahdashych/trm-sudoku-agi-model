# TRM for Sudoku-Extreme Configuration
# Optimized for L4 GPU (24GB VRAM)
# Based on "Less is More: Recursive Reasoning with Tiny Networks"

# Model Architecture
model:
  hidden_dim: 512          # Hidden dimension (paper: 512)
  n_latent: 6              # Latent recursions per deep step (paper: 6)
  T_deep: 3                # Deep recursion loops (paper: 3)
  use_act: true            # Adaptive Computation Time
  mlp_ratio: 2.666667      # 8/3 for SwiGLU

# Training Configuration
training:
  # Epochs and samples
  epochs: 200              # Reduced - best results were around epoch 90-100
  train_samples: 1000      # Same as paper (small-sample learning)
  augmentations_per_sample: 200  # Paper: 1000, gives 200K samples/epoch

  # Optimization (tuned for batch_size=8192)
  # With 200K samples / 8192 batch = ~24 steps per epoch
  batch_size: 8192         # Optimized for L4 GPU
  learning_rate: 0.001     # Slightly lower for stability
  weight_decay: 1.0        # Back to paper value - prevents overfitting
  warmup_steps: 200        # ~8 epochs warmup
  max_grad_norm: 1.0       # Gradient clipping

  # Early stopping
  early_stopping: true
  early_stopping_patience: 15  # Stop if val_loss doesn't improve for 15 evals
  early_stopping_min_delta: 0.001  # Minimum improvement to count as "better"

  # EMA
  ema_decay: 0.999         # Same as paper
  ema_warmup_steps: 50

  # Mixed precision
  use_amp: true
  amp_dtype: float16       # L4 works better with FP16

# Data
data:
  dataset_name: "Ritvik19/Sudoku-Dataset"
  num_workers: 4
  seed: 42
  test_samples: 1000       # Limit test samples to avoid memory issues

# Logging and Checkpointing
# Adjusted for ~24 steps per epoch
logging:
  log_interval: 24         # Log once per epoch
  eval_interval: 72        # Evaluate every ~3 epochs (more frequent for early stopping)
  save_interval: 240       # Save checkpoint every ~10 epochs
  output_dir: outputs

# Evaluation
evaluation:
  test_samples: null       # null = all test samples
  llm_comparison_samples: 5

# LLM Comparison (Ollama)
llm:
  model: "llama3.2"        # or "mistral", "qwen2.5"
  ollama_url: "http://localhost:11434"
  strategies:
    - direct
    - cot
    - fewshot

# Device
device: cuda               # cuda or cpu
